# KG-RAG Configuration
# Copy this file to .env and fill in your values

# =============================================================================
# NEO4J CONFIGURATION
# =============================================================================
NEO4J_URI=bolt://localhost:7687
NEO4J_USER=neo4j
NEO4J_PASSWORD=password

# =============================================================================
# LLM CONFIGURATION (via LiteLLM)
# =============================================================================
# Default model to use (supports OpenAI, Anthropic, Ollama, etc.)
# Examples:
#   - gpt-4o-mini (OpenAI)
#   - claude-3-5-sonnet-20241022 (Anthropic)
#   - ollama/llama3.2 (Local Ollama)
DEFAULT_LLM_MODEL=gpt-4o-mini

# API Keys (only set the ones you need)
OPENAI_API_KEY=
ANTHROPIC_API_KEY=

# For local Ollama (no API key needed, just the base URL)
OLLAMA_BASE_URL=http://localhost:11434

# =============================================================================
# EXTRACTION CONFIGURATION
# =============================================================================
# Model specifically for extraction (can be different from chat model)
EXTRACTION_MODEL=gpt-4o-mini
# Temperature for extraction (lower = more deterministic)
EXTRACTION_TEMPERATURE=0.0
# Max tokens for extraction responses
EXTRACTION_MAX_TOKENS=4096

# =============================================================================
# RAG CONFIGURATION
# =============================================================================
# Model for RAG response generation
RAG_MODEL=gpt-4o-mini
RAG_TEMPERATURE=0.7
RAG_MAX_TOKENS=2048

# =============================================================================
# CHUNKING CONFIGURATION
# =============================================================================
CHUNK_SIZE=1000
CHUNK_OVERLAP=200

# =============================================================================
# SCHEMA CONFIGURATION
# =============================================================================
# Which schema to use for extraction (filename without .yaml)
# Available: contract, research_paper (or create your own)
ACTIVE_SCHEMA=contract

# =============================================================================
# APPLICATION SETTINGS
# =============================================================================
# Backend
BACKEND_HOST=0.0.0.0
BACKEND_PORT=8000
DEBUG=true

# Frontend
VITE_API_URL=http://localhost:8000

# =============================================================================
# OPTIONAL: VECTOR STORE (for hybrid retrieval)
# =============================================================================
# EMBEDDING_MODEL=text-embedding-3-small
# VECTOR_STORE_TYPE=chroma  # or qdrant, pinecone
